{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiveKit Voice Agent with Comprehensive Metrics\n",
    "\n",
    "This notebook contains a complete voice agent implementation that:\n",
    "- Listens to user speech (STT - Speech to Text)\n",
    "- Processes text with AI (LLM - Large Language Model)\n",
    "- Responds with synthesized speech (TTS - Text to Speech)\n",
    "- Tracks detailed performance metrics\n",
    "\n",
    "## Required API Keys\n",
    "Before running, make sure you have:\n",
    "- OpenAI API key (for GPT and Whisper)\n",
    "- ElevenLabs API key (for voice synthesis)\n",
    "\n",
    "Add these to a `.env` file in your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Requirements\n",
    "\n",
    "Run this cell first to install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install livekit-agents livekit-plugins-openai livekit-plugins-elevenlabs livekit-plugins-silero python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the specified .env file\n",
    "# This file should contain your API keys (OPENAI_API_KEY, ELEVEN_API_KEY)\n",
    "# The override=True parameter ensures that environment variables are \n",
    "# overwritten if they're already set\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Configure logging for the DeepLearning.AI agent\n",
    "# This helps track the agent's behavior and debug issues\n",
    "logger = logging.getLogger(\"dlai-agent\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Add console handler for notebook output\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LiveKit Voice Agent Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livekit import agents\n",
    "from livekit.agents import Agent, AgentSession, JobContext, WorkerOptions, jupyter\n",
    "from livekit.plugins import (\n",
    "    openai,      # For LLM (Language Model) and STT (Speech-to-Text)\n",
    "    elevenlabs,  # For TTS (Text-to-Speech) - high quality voice synthesis\n",
    "    silero,      # For VAD (Voice Activity Detection) - detects when user is speaking\n",
    ")\n",
    "from livekit.agents.metrics import LLMMetrics, STTMetrics, TTSMetrics, EOUMetrics\n",
    "import asyncio\n",
    "\n",
    "print(\"LiveKit imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Voice Agent Class with Comprehensive Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsAgent(Agent):\n",
    "    \"\"\"\n",
    "    A voice agent that can:\n",
    "    1. Listen to user speech (STT - Speech to Text)\n",
    "    2. Process the text with an AI model (LLM - Large Language Model)\n",
    "    3. Respond with synthesized speech (TTS - Text to Speech)\n",
    "    4. Track detailed performance metrics for all components\n",
    "    \n",
    "    This agent is designed for real-time voice conversations with comprehensive\n",
    "    performance monitoring to help optimize latency and quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the voice agent with all necessary components.\n",
    "        \n",
    "        Components initialized:\n",
    "        - LLM: Language model for generating responses\n",
    "        - STT: Speech-to-text for understanding user input\n",
    "        - TTS: Text-to-speech for voice responses\n",
    "        - VAD: Voice activity detection for better conversation flow\n",
    "        \"\"\"\n",
    "        \n",
    "        # API KEY VALIDATION\n",
    "        \n",
    "        # Check if OpenAI API key is available\n",
    "        # This is required for both LLM (GPT) and STT (Whisper) functionality\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\n",
    "                \"OPENAI_API_KEY environment variable is not set. \"\n",
    "                \"Please add your OpenAI API key to the .env file.\"\n",
    "            )\n",
    "        \n",
    "        # Check if ElevenLabs API key is available\n",
    "        # This is required for high-quality text-to-speech synthesis\n",
    "        if not os.getenv(\"ELEVEN_API_KEY\"):\n",
    "            raise ValueError(\n",
    "                \"ELEVEN_API_KEY environment variable is not set. \"\n",
    "                \"Please add your ElevenLabs API key to the .env file.\"\n",
    "            )\n",
    "        \n",
    "        # COMPONENT INITIALIZATION\n",
    "        \n",
    "        # Initialize the Language Model (LLM)\n",
    "        # Using gpt-4o-mini for cost efficiency while maintaining good quality\n",
    "        # Alternative: gpt-4o for higher quality but increased cost\n",
    "        llm = openai.LLM(model=\"gpt-4o-mini\")\n",
    "        \n",
    "        # Initialize Speech-to-Text (STT)\n",
    "        # Whisper-1 is OpenAI's speech recognition model\n",
    "        # It supports multiple languages and handles various audio qualities well\n",
    "        stt = openai.STT(model=\"whisper-1\")\n",
    "        \n",
    "        # Initialize Text-to-Speech (TTS)\n",
    "        # ElevenLabs provides high-quality, natural-sounding voice synthesis\n",
    "        try:\n",
    "            tts = elevenlabs.TTS()\n",
    "        except Exception as e:\n",
    "            # Log the error with detailed information for debugging\n",
    "            logger.error(f\"Failed to initialize ElevenLabs TTS: {e}\")\n",
    "            logger.error(\"This usually means the API key is invalid or network issues\")\n",
    "            raise\n",
    "        \n",
    "        # Initialize Voice Activity Detection (VAD)\n",
    "        # Silero VAD helps detect when the user is speaking vs. silent\n",
    "        # This improves conversation flow by reducing false triggers\n",
    "        silero_vad = silero.VAD.load()\n",
    "        \n",
    "        # AGENT INITIALIZATION\n",
    "        \n",
    "        # Initialize the parent Agent class with all components\n",
    "        super().__init__(\n",
    "            # Instructions define the agent's personality and behavior\n",
    "            instructions=(\n",
    "                \"You are a helpful assistant communicating via voice. \"\n",
    "                \"Keep responses concise and natural for spoken conversation. \"\n",
    "                \"Avoid overly long responses that might lose the user's attention. \"\n",
    "                \"Be conversational and friendly.\"\n",
    "            ),\n",
    "            stt=stt,           # Speech-to-text component\n",
    "            llm=llm,           # Language model component\n",
    "            tts=tts,           # Text-to-speech component\n",
    "            vad=silero_vad,    # Voice activity detection component\n",
    "        )\n",
    "        \n",
    "        # Set up metrics collection for performance monitoring\n",
    "        self._setup_metrics_callbacks()\n",
    "    \n",
    "    def _setup_metrics_callbacks(self):\n",
    "        \"\"\"\n",
    "        Set up event listeners for metrics collection from all components.\n",
    "        \n",
    "        This allows us to track:\n",
    "        - LLM performance (token usage, response time)\n",
    "        - STT performance (transcription accuracy, latency)\n",
    "        - TTS performance (synthesis speed, audio quality)\n",
    "        - End-of-utterance detection performance\n",
    "        \n",
    "        Metrics help identify bottlenecks and optimize the user experience.\n",
    "        \"\"\"\n",
    "        \n",
    "        # LLM Metrics Callback\n",
    "        # Tracks language model performance (tokens, speed, time-to-first-token)\n",
    "        def llm_metrics_wrapper(metrics: LLMMetrics):\n",
    "            # Use asyncio.create_task to handle async callback without blocking\n",
    "            asyncio.create_task(self.on_llm_metrics_collected(metrics))\n",
    "        self.llm.on(\"metrics_collected\", llm_metrics_wrapper)\n",
    "        \n",
    "        # STT Metrics Callback\n",
    "        # Tracks speech-to-text performance (transcription time, accuracy)\n",
    "        def stt_metrics_wrapper(metrics: STTMetrics):\n",
    "            asyncio.create_task(self.on_stt_metrics_collected(metrics))\n",
    "        self.stt.on(\"metrics_collected\", stt_metrics_wrapper)\n",
    "        \n",
    "        # End-of-Utterance (EOU) Metrics Callback\n",
    "        # Tracks how quickly the system detects when user stops speaking\n",
    "        def eou_metrics_wrapper(metrics: EOUMetrics):\n",
    "            asyncio.create_task(self.on_eou_metrics_collected(metrics))\n",
    "        self.stt.on(\"eou_metrics_collected\", eou_metrics_wrapper)\n",
    "        \n",
    "        # TTS Metrics Callback\n",
    "        # Tracks text-to-speech performance (synthesis speed, audio generation time)\n",
    "        def tts_metrics_wrapper(metrics: TTSMetrics):\n",
    "            asyncio.create_task(self.on_tts_metrics_collected(metrics))\n",
    "        self.tts.on(\"metrics_collected\", tts_metrics_wrapper)\n",
    "    \n",
    "    # METRICS COLLECTION HANDLERS\n",
    "    \n",
    "    async def on_llm_metrics_collected(self, metrics: LLMMetrics) -> None:\n",
    "        \"\"\"\n",
    "        Handle Language Model metrics collection.\n",
    "        \n",
    "        Key metrics tracked:\n",
    "        - prompt_tokens: Number of tokens in the user's input\n",
    "        - completion_tokens: Number of tokens in the AI's response\n",
    "        - tokens_per_second: Speed of token generation (higher = faster)\n",
    "        - ttft: Time To First Token (lower = more responsive)\n",
    "        \n",
    "        These metrics help optimize cost (token usage) and user experience (speed).\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"LLM Metrics - Prompt: %d tokens, Completion: %d tokens, \"\n",
    "            \"Speed: %.4f tok/s, TTFT: %.4f s\",\n",
    "            metrics.prompt_tokens, \n",
    "            metrics.completion_tokens, \n",
    "            metrics.tokens_per_second, \n",
    "            metrics.ttft\n",
    "        )\n",
    "    \n",
    "    async def on_stt_metrics_collected(self, metrics: STTMetrics) -> None:\n",
    "        \"\"\"\n",
    "        Handle Speech-to-Text metrics collection.\n",
    "        \n",
    "        Key metrics tracked:\n",
    "        - duration: Total time for speech recognition process\n",
    "        - audio_duration: Length of the audio that was processed\n",
    "        - streamed: Whether the transcription was streamed (real-time) or batch\n",
    "        \n",
    "        These metrics help optimize transcription latency and quality.\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"STT Metrics - Processing: %.4f s, Audio Length: %.4f s, \"\n",
    "            \"Streamed: %s\",\n",
    "            metrics.duration, \n",
    "            metrics.audio_duration, \n",
    "            \"Yes\" if metrics.streamed else \"No\"\n",
    "        )\n",
    "    \n",
    "    async def on_eou_metrics_collected(self, metrics: EOUMetrics) -> None:\n",
    "        \"\"\"\n",
    "        Handle End-of-Utterance metrics collection.\n",
    "        \n",
    "        Key metrics tracked:\n",
    "        - end_of_utterance_delay: Time to detect user stopped speaking\n",
    "        - transcription_delay: Time from end of speech to transcription completion\n",
    "        \n",
    "        These metrics are crucial for conversation flow - shorter delays mean\n",
    "        more natural conversation rhythm.\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"EOU Metrics - End Detection: %.4f s, Transcription Delay: %.4f s\",\n",
    "            metrics.end_of_utterance_delay, \n",
    "            metrics.transcription_delay\n",
    "        )\n",
    "    \n",
    "    async def on_tts_metrics_collected(self, metrics: TTSMetrics) -> None:\n",
    "        \"\"\"\n",
    "        Handle Text-to-Speech metrics collection.\n",
    "        \n",
    "        Key metrics tracked:\n",
    "        - ttfb: Time To First Byte of audio (lower = more responsive)\n",
    "        - duration: Total time for speech synthesis\n",
    "        - audio_duration: Length of the generated audio\n",
    "        - streamed: Whether audio was streamed or generated as a complete file\n",
    "        \n",
    "        These metrics help optimize response time and audio quality.\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            \"TTS Metrics - TTFB: %.4f s, Processing: %.4f s, \"\n",
    "            \"Audio Length: %.4f s, Streamed: %s\",\n",
    "            metrics.ttfb, \n",
    "            metrics.duration, \n",
    "            metrics.audio_duration, \n",
    "            \"Yes\" if metrics.streamed else \"No\"\n",
    "        )\n",
    "\n",
    "print(\"MetricsAgent class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Entrypoint Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def entrypoint(ctx: JobContext):\n",
    "    \"\"\"\n",
    "    Main entrypoint function for the voice agent.\n",
    "    \n",
    "    This function is called by LiveKit when a new session starts.\n",
    "    It handles:\n",
    "    1. Connecting to the LiveKit room\n",
    "    2. Creating and starting the agent session\n",
    "    3. Error handling and logging\n",
    "    \n",
    "    Args:\n",
    "        ctx (JobContext): LiveKit job context containing room and connection info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the LiveKit room\n",
    "        # This establishes the WebRTC connection for real-time audio\n",
    "        await ctx.connect()\n",
    "        logger.info(\"Successfully connected to LiveKit room\")\n",
    "        \n",
    "        # Create a new agent session\n",
    "        # This manages the lifecycle of the conversation\n",
    "        session = AgentSession()\n",
    "        \n",
    "        # Create an instance of our MetricsAgent\n",
    "        agent = MetricsAgent()\n",
    "        logger.info(\"MetricsAgent initialized successfully\")\n",
    "        \n",
    "        # Start the agent session\n",
    "        # This begins listening for audio and handling conversations\n",
    "        await session.start(\n",
    "            agent=agent,\n",
    "            room=ctx.room,\n",
    "        )\n",
    "        logger.info(\"Agent session started successfully - ready for conversations\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log any errors that occur during initialization\n",
    "        logger.error(f\"Error in entrypoint: {e}\")\n",
    "        logger.error(\"This could be due to network issues, invalid API keys, or room connection problems\")\n",
    "        raise\n",
    "\n",
    "print(\"Entrypoint function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-flight Checks and Agent Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-FLIGHT CHECKS\n",
    "\n",
    "# Verify OpenAI API key is available\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"ERROR: OPENAI_API_KEY not found in environment variables\")\n",
    "    print(\"Please add your OpenAI API key to the .env file:\")\n",
    "    print(\"OPENAI_API_KEY=your_api_key_here\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "\n",
    "# Verify ElevenLabs API key is available\n",
    "if not os.getenv(\"ELEVEN_API_KEY\"):\n",
    "    print(\"ERROR: ELEVEN_API_KEY not found in environment variables\")\n",
    "    print(\"Please add your ElevenLabs API key to the .env file:\")\n",
    "    print(\"ELEVEN_API_KEY=your_api_key_here\")\n",
    "else:\n",
    "    print(\"‚úÖ ElevenLabs API key found\")\n",
    "\n",
    "# Check if both keys are available\n",
    "if os.getenv(\"OPENAI_API_KEY\") and os.getenv(\"ELEVEN_API_KEY\"):\n",
    "    print(\"\\n‚úÖ All API keys loaded successfully\")\n",
    "    print(\"üöÄ Ready to start voice agent...\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Please set up your API keys before proceeding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Voice Agent\n",
    "\n",
    "**Important:** This cell will start the voice agent and open a web interface for testing. Make sure your API keys are properly configured before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if API keys are available\n",
    "if os.getenv(\"OPENAI_API_KEY\") and os.getenv(\"ELEVEN_API_KEY\"):\n",
    "    print(\"Starting LiveKit Voice Agent...\")\n",
    "    print(\"This will open a web interface for testing the voice agent.\")\n",
    "    \n",
    "    # Run the LiveKit agent application\n",
    "    # WorkerOptions configures how the agent worker behaves\n",
    "    # The jupyter_url provides a web interface for testing and debugging\n",
    "    try:\n",
    "        jupyter.run_app(\n",
    "            WorkerOptions(entrypoint_fnc=entrypoint), \n",
    "            jupyter_url=\"https://jupyter-api-livekit.vercel.app/api/join-token\"\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nAgent stopped by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError starting agent: {e}\")\n",
    "        print(\"Check your internet connection and API keys\")\nelse:\n",
    "    print(\"Please configure your API keys first!\")\n",
    "    print(\"Add them to a .env file in the same directory as this notebook:\")\n",
    "    print(\"OPENAI_API_KEY=your_openai_key_here\")\n",
    "    print(\"ELEVEN_API_KEY=your_elevenlabs_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Setup Environment:**\n",
    "   - Create a `.env` file in the same directory as this notebook\n",
    "   - Add your API keys:\n",
    "     ```\n",
    "     OPENAI_API_KEY=your_openai_api_key_here\n",
    "     ELEVEN_API_KEY=your_elevenlabs_api_key_here\n",
    "     ```\n",
    "\n",
    "2. **Run the Cells:**\n",
    "   - Execute cells in order from top to bottom\n",
    "   - The installation cell only needs to be run once\n",
    "   - The final cell will start the voice agent\n",
    "\n",
    "3. **Test the Agent:**\n",
    "   - When the agent starts, it will provide a web interface URL\n",
    "   - Click the URL to open the testing interface\n",
    "   - Allow microphone access when prompted\n",
    "   - Start speaking to interact with the voice agent\n",
    "\n",
    "4. **Monitor Metrics:**\n",
    "   - Watch the notebook output for detailed performance metrics\n",
    "   - Metrics include response times, token usage, and audio processing stats\n",
    "   - Use these metrics to optimize performance\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **API Key Errors:** Double-check your `.env` file format and key validity\n",
    "- **Connection Issues:** Ensure stable internet connection\n",
    "- **Audio Problems:** Check browser microphone permissions\n",
    "- **Performance Issues:** Monitor the metrics output for bottlenecks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
